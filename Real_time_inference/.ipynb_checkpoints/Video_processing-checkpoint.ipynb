{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd2bec7-373a-4909-a68b-8cb4dfd6c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e57d29-eed2-4d85-8919-24aab890611a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np \n",
    "\n",
    "model=load_model('/home/emanuele/ecografia/dati/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37885486-2407-4e53-ad32-8796029d2c0e",
   "metadata": {},
   "source": [
    "# Quantizzo modello "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b529e1-99e0-4d57-a74a-1255d139102a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Converti il modello in un modello TensorFlow Lite quantizzato\\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\\nquantized_tflite_model = converter.convert()\\n\\n# Salva il modello quantizzato su disco\\nwith open('/users/emanueleamato/Downloads/Esame Echo Emanuele /20240215/model_quantized.tflite', 'wb') as f:\\n    f.write(quantized_tflite_model)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Converti il modello in un modello TensorFlow Lite quantizzato\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Salva il modello quantizzato su disco\n",
    "with open('/users/emanueleamato/Downloads/Esame Echo Emanuele /20240215/model_quantized.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365bc9b1-1b06-4baf-879e-353a8e976ef6",
   "metadata": {},
   "source": [
    "# Frame evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa4b803-c2dc-4c17-8062-1b2d7ee8d88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = tf.lite.Interpreter(model_path=\"/home/emanuele/ecografia/dati/model_quantized.tflite\")\\nmodel.allocate_tensors()\\n\\n# Apre il video\\nvideo_capture = cv2.VideoCapture(\\'/home/emanuele/ecografia/dati/Image64.mp4\\')\\n\\n# Definisce una funzione per preprocessare i frame\\ndef preprocess_frame(frame):\\n    resized_frame = cv2.resize(frame, (256, 256))\\n    normalized_frame = resized_frame / 255.0\\n    preprocessed_frame = np.expand_dims(normalized_frame.astype(np.float32), axis=0) \\n    return preprocessed_frame\\n\\n# Esegue l\\'inferenza su un numero limitato di frame\\n\\nlabels = [\"Good\", \"Medium\", \"Poor\"]\\nframe_count = 0\\nmax_frames = 15  # Numero massimo di frame su cui eseguire l\\'inferenza\\nwhile True:\\n    ret, frame = video_capture.read()\\n    \\n    if not ret:\\n        break\\n    \\n    processed_frame = preprocess_frame(frame)\\n\\n    # Esegue l\\'inferenza solo su un sottoinsieme di frame\\n    if frame_count < max_frames:\\n        model.set_tensor(model.get_input_details()[0][\\'index\\'], processed_frame)\\n        model.invoke()\\n\\n        output_details = model.get_output_details()[0]\\n        output_data = model.get_tensor(output_details[\\'index\\'])\\n\\n        predicted_class_index = np.argmax(output_data)\\n       \\n        predicted_class_name = labels[predicted_class_index]\\n        max_probability = np.max(output_data)\\n\\n        print(f\"Frame {frame_count + 1}: Class: {predicted_class_name}, Probability: {max_probability}\")\\n\\n        text = f\"Class: {predicted_class_name}, Probability: {max_probability}\"\\n        cv2.putText(frame, text, (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\\n        \\n        # Mostra il frame con le previsioni\\n        cv2.imshow(\\'Frame\\', frame)\\n        cv2.waitKey(1000)  # Mostra il frame per 1 secondo\\n\\n\\n    frame_count += 1\\n\\n    if frame_count >= max_frames:\\n        break\\n\\n# Rilascia la cattura del video\\nvideo_capture.release()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = tf.lite.Interpreter(model_path=\"/home/emanuele/ecografia/dati/model_quantized.tflite\")\n",
    "model.allocate_tensors()\n",
    "\n",
    "# Apre il video\n",
    "video_capture = cv2.VideoCapture('/home/emanuele/ecografia/dati/Image64.mp4')\n",
    "\n",
    "# Definisce una funzione per preprocessare i frame\n",
    "def preprocess_frame(frame):\n",
    "    resized_frame = cv2.resize(frame, (256, 256))\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "    preprocessed_frame = np.expand_dims(normalized_frame.astype(np.float32), axis=0) \n",
    "    return preprocessed_frame\n",
    "\n",
    "# Esegue l'inferenza su un numero limitato di frame\n",
    "\n",
    "labels = [\"Good\", \"Medium\", \"Poor\"]\n",
    "frame_count = 0\n",
    "max_frames = 15  # Numero massimo di frame su cui eseguire l'inferenza\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    processed_frame = preprocess_frame(frame)\n",
    "\n",
    "    # Esegue l'inferenza solo su un sottoinsieme di frame\n",
    "    if frame_count < max_frames:\n",
    "        model.set_tensor(model.get_input_details()[0]['index'], processed_frame)\n",
    "        model.invoke()\n",
    "\n",
    "        output_details = model.get_output_details()[0]\n",
    "        output_data = model.get_tensor(output_details['index'])\n",
    "\n",
    "        predicted_class_index = np.argmax(output_data)\n",
    "       \n",
    "        predicted_class_name = labels[predicted_class_index]\n",
    "        max_probability = np.max(output_data)\n",
    "\n",
    "        print(f\"Frame {frame_count + 1}: Class: {predicted_class_name}, Probability: {max_probability}\")\n",
    "\n",
    "        text = f\"Class: {predicted_class_name}, Probability: {max_probability}\"\n",
    "        cv2.putText(frame, text, (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Mostra il frame con le previsioni\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.waitKey(1000)  # Mostra il frame per 1 secondo\n",
    "\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    if frame_count >= max_frames:\n",
    "        break\n",
    "\n",
    "# Rilascia la cattura del video\n",
    "video_capture.release()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab959d-77b5-4a0b-bf20-36a9743248a1",
   "metadata": {},
   "source": [
    "# Video Fissato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53511893-85c6-4aab-b705-a72ac7ba10c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport cv2\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import load_model\\nimport numpy as np \\n\\n\\nmodel = tf.lite.Interpreter(model_path=\"/home/emanuele/ecografia/dati/model_quantized.tflite\")\\nmodel.allocate_tensors()\\n\\n# Apre il video\\nvideo_capture = cv2.VideoCapture(\\'/home/emanuele/ecografia/dati/Image64.mp4\\')\\n\\n# Definisce una funzione per preprocessare i frame\\ndef preprocess_frame(frame):\\n    resized_frame = cv2.resize(frame, (256, 256))\\n    normalized_frame = resized_frame / 255.0\\n    preprocessed_frame = np.expand_dims(normalized_frame.astype(np.float32), axis=0) \\n    return preprocessed_frame\\n\\n\\n# Ottiene le informazioni sui frame del video sorgente\\nframe_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\\nframe_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\\nframe_rate = int(video_capture.get(cv2.CAP_PROP_FPS))\\n\\n# Definisce un oggetto VideoWriter per scrivere il nuovo video con le previsioni sovrapposte\\noutput_video_path = \\'/home/emanuele/ecografia/dati/Image64_WWOW.mp4\\'\\nfourcc = cv2.VideoWriter_fourcc(*\\'mp4v\\')\\noutput_video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\\n\\n# Esegue l\\'inferenza su ogni frame del video e mostra il video con le previsioni sovrapposte in tempo reale\\nlabels = [\"Good\", \"Medium\", \"Poor\"]\\nwhile True:\\n    ret, frame = video_capture.read()\\n    \\n    if not ret:\\n        break\\n    \\n    # Esegue l\\'inferenza sul frame corrente\\n    processed_frame = preprocess_frame(frame)\\n    model.set_tensor(model.get_input_details()[0][\\'index\\'], processed_frame)\\n    model.invoke()\\n    output_details = model.get_output_details()[0]\\n    output_data = model.get_tensor(output_details[\\'index\\'])\\n    predicted_class_index = np.argmax(output_data)\\n    predicted_class_name = labels[predicted_class_index]\\n    max_probability = np.max(output_data)\\n\\n    # Scrive la previsione sul frame\\n    text = f\"Class: {predicted_class_name}, Probability: {max_probability}\"\\n    cv2.putText(frame, text, (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\\n\\n    # Aggiunge il frame con le previsioni al video di output\\n    output_video.write(frame)\\n\\n    # Mostra il frame con le previsioni sovrapposte\\n    cv2.imshow(\\'Real-Time Video with Predictions\\', frame)\\n\\n    # Interrompe l\\'esecuzione se viene premuto il tasto \\'q\\'\\n    if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break\\n\\n# Rilascia le risorse\\nvideo_capture.release()\\noutput_video.release()\\ncv2.destroyAllWindows()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "model = tf.lite.Interpreter(model_path=\"/home/emanuele/ecografia/dati/model_quantized.tflite\")\n",
    "model.allocate_tensors()\n",
    "\n",
    "# Apre il video\n",
    "video_capture = cv2.VideoCapture('/home/emanuele/ecografia/dati/Image64.mp4')\n",
    "\n",
    "# Definisce una funzione per preprocessare i frame\n",
    "def preprocess_frame(frame):\n",
    "    resized_frame = cv2.resize(frame, (256, 256))\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "    preprocessed_frame = np.expand_dims(normalized_frame.astype(np.float32), axis=0) \n",
    "    return preprocessed_frame\n",
    "\n",
    "\n",
    "# Ottiene le informazioni sui frame del video sorgente\n",
    "frame_width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Definisce un oggetto VideoWriter per scrivere il nuovo video con le previsioni sovrapposte\n",
    "output_video_path = '/home/emanuele/ecografia/dati/Image64_WWOW.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "# Esegue l'inferenza su ogni frame del video e mostra il video con le previsioni sovrapposte in tempo reale\n",
    "labels = [\"Good\", \"Medium\", \"Poor\"]\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Esegue l'inferenza sul frame corrente\n",
    "    processed_frame = preprocess_frame(frame)\n",
    "    model.set_tensor(model.get_input_details()[0]['index'], processed_frame)\n",
    "    model.invoke()\n",
    "    output_details = model.get_output_details()[0]\n",
    "    output_data = model.get_tensor(output_details['index'])\n",
    "    predicted_class_index = np.argmax(output_data)\n",
    "    predicted_class_name = labels[predicted_class_index]\n",
    "    max_probability = np.max(output_data)\n",
    "\n",
    "    # Scrive la previsione sul frame\n",
    "    text = f\"Class: {predicted_class_name}, Probability: {max_probability}\"\n",
    "    cv2.putText(frame, text, (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Aggiunge il frame con le previsioni al video di output\n",
    "    output_video.write(frame)\n",
    "\n",
    "    # Mostra il frame con le previsioni sovrapposte\n",
    "    cv2.imshow('Real-Time Video with Predictions', frame)\n",
    "\n",
    "    # Interrompe l'esecuzione se viene premuto il tasto 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Rilascia le risorse\n",
    "video_capture.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de3e2e-1ed8-4083-857e-a65e868b9d54",
   "metadata": {},
   "source": [
    "# Duplicazione schermo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3888ed-3b74-4a3c-9a0a-5f2932822685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Carica il modello TensorFlow Lite\n",
    "model = tf.lite.Interpreter(model_path=\"model_quantized.tflite\")\n",
    "model.allocate_tensors()\n",
    "\n",
    "# Definisci una funzione per preprocessare i frame\n",
    "def preprocess_frame(frame):\n",
    "    resized_frame = cv2.resize(frame, (224, 224))\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "    preprocessed_frame = np.expand_dims(normalized_frame.astype(np.float32), axis=0) \n",
    "    return preprocessed_frame\n",
    "\n",
    "\n",
    "\n",
    "# Apri la sorgente video (ad esempio, una duplicazione dello schermo)\n",
    "video_capture = cv2.VideoCapture(0)  # Modifica il numero per utilizzare una sorgente video diversa\n",
    "\n",
    "while True:\n",
    "    # Leggi il frame dalla sorgente video\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocessa il frame\n",
    "    processed_frame = preprocess_frame(frame)\n",
    "\n",
    "    # Esegui l'inferenza sul frame\n",
    "    model.set_tensor(model.get_input_details()[0]['index'], processed_frame)\n",
    "    model.invoke()\n",
    "\n",
    "    # Ottieni l'output dell'inferenza\n",
    "    output_details = model.get_output_details()[0]\n",
    "    output_data = model.get_tensor(output_details['index'])\n",
    "\n",
    "    # Trova la classe predetta e la probabilità massima\n",
    "    predicted_class_index = np.argmax(output_data)\n",
    "    labels = [\"2CH\", \"4CH\"]  # Sostituisci con le tue etichette di classe\n",
    "    predicted_class_name = labels[predicted_class_index]\n",
    "    max_probability = np.max(output_data)\n",
    "\n",
    "    # Disegna il testo sul frame con il nome della classe predetta e la probabilità\n",
    "    text = f\"Class: {predicted_class_name}, Probability: {max_probability}\"\n",
    "    cv2.putText(frame, text, (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Mostra il frame con le previsioni\n",
    "    cv2.imshow('Frame', frame)\n",
    "    \n",
    "    # Esci se viene premuto il tasto 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Rilascia la cattura del video e chiudi le finestre\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e28d56a-b03b-451f-b068-98c52a378fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pyautogui \\nimport cv2\\nimport numpy as np\\nimport tensorflow as tf\\n\\n# Carica il modello TensorFlow Lite\\nmodel = tf.lite.Interpreter(model_path=\"model_quantized.tflite\")\\nmodel.allocate_tensors()\\n\\n# Definisci una funzione per preprocessare i frame\\ndef preprocess_frame(frame):\\n    resized_frame = cv2.resize(frame, (256, 256))\\n    normalized_frame = resized_frame / 255.0\\n    preprocessed_frame = np.expand_dims(normalized_frame.astype(np.float32), axis=0) \\n    return preprocessed_frame\\n\\n\\nwhile True:\\n    # Leggi il frame dalla sorgente video\\n    screenshot=pyautogui.screenshot(region=(1920,1080,1920,1080))\\n    frame=np.array(screenshot)\\n    preprocessed_frame=preprocess_frame(frame)\\n\\n    input_details=model.get_input_details()\\n    output_details=model.get_output_details()\\n    model.set_tensor(input_details[0][\\'index\\'],preprocessed_frame)\\n    model.invoke()\\n\\n    output_data=model.get_tensor(output_details[0][\\'index\\'])\\n    predicted_class_index = np.argmax(output_data)\\n    labels = [\"Good\", \"Medium\", \"Poor\"]  # Sostituisci con le tue etichette di classe\\n    predicted_class_name = labels[predicted_class_index]\\n    max_probability = np.max(output_data)\\n\\n    # Disegna il testo sul frame con il nome della classe predetta e la probabilità\\n    text = f\"Class: {predicted_class_name}, Probability: {max_probability}\"\\n    cv2.putText(frame, text, (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\\n    \\n    # Esci se viene premuto il tasto \\'q\\'\\n    if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break\\ncv2.destroyAllWindows()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pyautogui \n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Carica il modello TensorFlow Lite\n",
    "model = tf.lite.Interpreter(model_path=\"model_quantized.tflite\")\n",
    "model.allocate_tensors()\n",
    "\n",
    "# Definisci una funzione per preprocessare i frame\n",
    "def preprocess_frame(frame):\n",
    "    resized_frame = cv2.resize(frame, (256, 256))\n",
    "    normalized_frame = resized_frame / 255.0\n",
    "    preprocessed_frame = np.expand_dims(normalized_frame.astype(np.float32), axis=0) \n",
    "    return preprocessed_frame\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Leggi il frame dalla sorgente video\n",
    "    screenshot=pyautogui.screenshot(region=(1920,1080,1920,1080))\n",
    "    frame=np.array(screenshot)\n",
    "    preprocessed_frame=preprocess_frame(frame)\n",
    "\n",
    "    input_details=model.get_input_details()\n",
    "    output_details=model.get_output_details()\n",
    "    model.set_tensor(input_details[0]['index'],preprocessed_frame)\n",
    "    model.invoke()\n",
    "\n",
    "    output_data=model.get_tensor(output_details[0]['index'])\n",
    "    predicted_class_index = np.argmax(output_data)\n",
    "    labels = [\"Good\", \"Medium\", \"Poor\"]  # Sostituisci con le tue etichette di classe\n",
    "    predicted_class_name = labels[predicted_class_index]\n",
    "    max_probability = np.max(output_data)\n",
    "\n",
    "    # Disegna il testo sul frame con il nome della classe predetta e la probabilità\n",
    "    text = f\"Class: {predicted_class_name}, Probability: {max_probability}\"\n",
    "    cv2.putText(frame, text, (25, 25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Esci se viene premuto il tasto 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1182252-7798-42cf-8cc8-6f59b0a3c61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d7709-37e9-4bdc-8891-09bc12265273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
